actual = read.csv("data/raw/actual.csv")
combined = read.csv(file=paste(datadir,"golub_combined.csv",sep='/'))
#get boolean lists for each data set of which patients had which cancer
ALLtrainBool = (actual$cancer == 'ALL')[0:(ncol(train)-2)]
AMLtrainBool = !ALLtrainBool
#index the data with the boolean lists to get the ALL and AML data
ALLtrain = train[3:ncol(train)][ALLtrainBool]
AMLtrain = train[3:ncol(train)][AMLtrainBool]
t.stat = apply(X=train, MARGIN=1, FUN=function(X){t.test(as.numeric(X[ALLtrainBool]),as.numeric(X[AMLtrainBool]))$statistic})
df = apply(X=train, MARGIN=1, FUN=function(X){t.test(as.numeric(X[ALLtrainBool]),as.numeric(X[AMLtrainBool]))$parameter})
p.value = 2*(1 - pnorm(abs(t.stat)))
alpha = 0.2
m = length(p.value)
p.value = sort(p.value)
kBH = alpha * (1:m)/m
which(p.value <= kBH)
plot(p.value)
lines(kBH, lty=2)
plot(p.value[1:10])
lines(kBH, lty=2)
s = sample(1:m, 1000)
Rho = cor(t(combined[s,]))
rho = Rho[which(lower.tri(R))]
hist(rho, breaks=50)
x = seq(-8,8,0.1)
hist(t.stat, breaks=x)
lines(x, m*0.1*dt(x, df=32), lty=1, col=2)
p0 = 1
P0 = p.value
Phat = ecdf(p.value)(p.value)
FDRhat = p0 * P0 / pmax(Phat,1/m)
plot(p.value[1:10], m*Phat[1:10], "b", ylim=c(0,10))
lines(p.value[1:10],m*P0[1:10], lty=2)
legend(0,10,c("m*Phat","m*P0"),lty=c(1,2))
plot(p.value[1:10], FDRhat[1:10],"b")
abline(a=alpha0, b=0, lty=2)
#check for the variance of two groups if they are equal or different
#Apply quantile transformation to transform all t statistics to normal z-scores
plot(df)
p = seq(0,1,by=0.01)
hist(p.value, freq=F)
lines(p, dunif(p), lwd=2, col='red')
x = seq(-8,8,by=0.1)
hist(t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
#read more on order norm
#every t-statistic has a different number of df, transformation should be different for each
#if this works then next we can simulate the null proportion
orderNorm_obj = orderNorm(t.stat)
new_t.stat = predict(orderNorm_obj)
x = seq(-8,8,by=0.1)
hist(new_t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
#turn each group into a matrix and calculate row-wise variance of each gene
ALLtrainVars = rowVars(as.matrix(ALLtrain))
AMLtrainVars = rowVars(as.matrix(AMLtrain))
#find the difference in variance between each respective gene by group
varDiffs = ALLtrainVars - AMLtrainVars
varRatios = ALLtrainVars / AMLtrainVars
plot(varDiffs)
plot(varRatios)
#we want to now transform the data to form a gaussian curve
#in other words, we want the distribution of t statistics to be uniform
ALLtrans = data.frame(matrix(ncol = ncol(ALLtrain)))
names(ALLtrans) = names(ALLtrain)
for (i in 1:nrow(ALLtrain)) {
orderNorm_obj = orderNorm(as.numeric(ALLtrain[i,]))
ALLtrans = rbind(ALLtrans, predict(orderNorm_obj))
}
ALLtrans = ALLtrans[-c(1), ] #remove initial empty row
rownames(ALLtrans) = NULL
#same for AML
AMLtrans = data.frame(matrix(ncol = ncol(AMLtrain)))
names(AMLtrans) = names(AMLtrain)
for (i in 1:nrow(AMLtrain)) {
orderNorm_obj = orderNorm(as.numeric(AMLtrain[i,]))
AMLtrans = rbind(AMLtrans, predict(orderNorm_obj))
}
AMLtrans = AMLtrans[-c(1),] #remove empty row
rownames(AMLtrans) = NULL
#add common column to merge with
AMLtrans$Gene_Accession_Number = train$Gene_Accession_Number
ALLtrans$Gene_Accession_Number = train$Gene_Accession_Number
transformed = merge(ALLtrans, AMLtrans, by="Gene_Accession_Number")[,-c(1)] #remove Gene Accession column
head(transformed)
#t test on each gene by group
t.stat = apply(X=transformed, MARGIN=1, FUN=function(X){t.test(as.numeric(X[ALLtrainBool]),as.numeric(X[AMLtrainBool]))$statistic})
#degrees of freedom by gene
df = apply(X=transformed, MARGIN=1, FUN=function(X){t.test(as.numeric(X[ALLtrainBool]),as.numeric(X[AMLtrainBool]))$parameter})
p.value = 2*(1 - pnorm(abs(t.stat)))
plot(df)
p = seq(0,1,by=0.01)
hist(p.value, freq=F)
lines(p, dunif(p), lwd=2, col='red')
x = seq(-8,8,by=0.1)
hist(t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
setwd('..')
setwd('..')
datadir = "data/cleaned"
df1 = read.table(file=paste(datadir,"golub1",sep='/'))
df2 = read.table(file=paste(datadir,"golub2",sep='/'))
#test set
g1 = 3:22
g2 = 23:36
t.stat = apply(X=df1, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$statistic})
df = apply(X=df1, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$parameter})
p.value = 2*(1 - pnorm(abs(t.stat)))
#p values histogram
p = seq(0,1,by=0.01)
hist(p.value, freq=F)
lines(p, dunif(p), lwd=2, col='red')
#t statistics histograms
x = seq(-8,8,by=0.1)
hist(t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
#training set
g1 = 3:29
g2 = 30:40
t.stat = apply(X=df2, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$statistic})
p.value = 2*(1 - pnorm(abs(t.stat)))
#p values histogram
p = seq(0,1,by=0.01)
hist(p.value, freq=F)
lines(p, dunif(p), lwd=2, col='red')
#t statistics histogram
x = seq(-8,8,by=0.1)
hist(t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
#every t-statistic has a different number of df, transformation should be different for each
#if this works then next we can simulate the null proportion
orderNorm_obj = orderNorm(t.stat) #currently working w/ training data
train.t.stat = predict(orderNorm_obj)
test.t.stat = 2*(1 - pnorm(abs(new_t.stat)))
x = seq(-8,8,by=0.1)
hist(new_t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
setwd("~/Documents/GitHub/dsc180b-capstone/src/analysis")
setwd('..')
setwd('..')
datadir = "data/cleaned"
df1 = read.table(file=paste(datadir,"golub1",sep='/'))
df2 = read.table(file=paste(datadir,"golub2",sep='/'))
#test set
g1 = 3:22
g2 = 23:36
t.stat = apply(X=df1, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$statistic})
df = apply(X=df1, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$parameter})
p.value = 2*(1 - pnorm(abs(t.stat)))
#p values histogram
p = seq(0,1,by=0.01)
hist(p.value, freq=F)
lines(p, dunif(p), lwd=2, col='red')
#t statistics histograms
x = seq(-8,8,by=0.1)
hist(t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
#training set
g1 = 3:29
g2 = 30:40
t.stat = apply(X=df2, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$statistic})
p.value = 2*(1 - pnorm(abs(t.stat)))
#p values histogram
p = seq(0,1,by=0.01)
hist(p.value, freq=F)
lines(p, dunif(p), lwd=2, col='red')
#t statistics histogram
x = seq(-8,8,by=0.1)
hist(t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
#every t-statistic has a different number of df, transformation should be different for each
#if this works then next we can simulate the null proportion
orderNorm_obj = orderNorm(t.stat) #currently working w/ training data
train.t.stat = predict(orderNorm_obj)
test.t.stat = 2*(1 - pnorm(abs(new_t.stat)))
x = seq(-8,8,by=0.1)
hist(new_t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
x = seq(-8,8,by=0.1)
hist(train.t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
alpha = 0.2
m = length(train.p.value)
k = alpha * (1:m)/m
which(train.p.value <= k)
orderNorm_obj = orderNorm(t.stat) #currently working w/ training data
train.t.stat = predict(orderNorm_obj)
train.p.value = 2*(1 - pnorm(abs(new_t.stat)))
x = seq(-8,8,by=0.1)
hist(train.t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
alpha = 0.2
m = length(train.p.value)
k = alpha * (1:m)/m
which(train.p.value <= k)
train.p.value
train.p.value = 2*(1 - pnorm(abs(new_t.stat)))
setwd('..')
setwd('..')
datadir = "data/cleaned"
df1 = read.table(file=paste(datadir,"golub1",sep='/'))
df2 = read.table(file=paste(datadir,"golub2",sep='/'))
#test set
g1 = 3:22
g2 = 23:36
t.stat = apply(X=df1, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$statistic})
df = apply(X=df1, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$parameter})
p.value = 2*(1 - pnorm(abs(t.stat)))
#p values histogram
p = seq(0,1,by=0.01)
hist(p.value, freq=F)
lines(p, dunif(p), lwd=2, col='red')
#t statistics histograms
x = seq(-8,8,by=0.1)
hist(t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
#training set
g1 = 3:29
g2 = 30:40
t.stat = apply(X=df2, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$statistic})
p.value = 2*(1 - pnorm(abs(t.stat)))
#p values histogram
p = seq(0,1,by=0.01)
hist(p.value, freq=F)
lines(p, dunif(p), lwd=2, col='red')
#t statistics histogram
x = seq(-8,8,by=0.1)
hist(t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
#every t-statistic has a different number of df, transformation should be different for each
#if this works then next we can simulate the null proportion
orderNorm_obj = orderNorm(t.stat) #currently working w/ training data
train.t.stat = predict(orderNorm_obj)
train.p.value = 2*(1 - pnorm(abs(new.t.stat)))
x = seq(-8,8,by=0.1)
hist(train.t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
train.p.value
#estimation of p0
alpha = 0.2
m = length(train.p.value)
k = alpha * (1:m)/m
which(train.p.value <= k)
setwd("~/Documents/GitHub/dsc180b-capstone/src/analysis")
setwd('..')
setwd('..')
datadir = "data/cleaned"
df1 = read.table(file=paste(datadir,"golub1",sep='/'))
df2 = read.table(file=paste(datadir,"golub2",sep='/'))
#test set
g1 = 3:22
g2 = 23:36
t.stat = apply(X=df1, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$statistic})
df = apply(X=df1, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$parameter})
p.value = 2*(1 - pnorm(abs(t.stat)))
#p values histogram
p = seq(0,1,by=0.01)
hist(p.value, freq=F)
lines(p, dunif(p), lwd=2, col='red')
#t statistics histograms
x = seq(-8,8,by=0.1)
hist(t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
#training set
g1 = 3:29
g2 = 30:40
t.stat = apply(X=df2, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$statistic})
p.value = 2*(1 - pnorm(abs(t.stat)))
#p values histogram
p = seq(0,1,by=0.01)
hist(p.value, freq=F)
lines(p, dunif(p), lwd=2, col='red')
#t statistics histogram
x = seq(-8,8,by=0.1)
hist(t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
#every t-statistic has a different number of df, transformation should be different for each
#if this works then next we can simulate the null proportion
orderNorm_obj = orderNorm(t.stat) #currently working w/ training data
train.t.stat = predict(orderNorm_obj)
train.p.value = 2*(1 - pnorm(abs(new.t.stat)))
x = seq(-8,8,by=0.1)
hist(train.t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
train.p.value
#estimation of p0
alpha = 0.2
m = length(train.p.value)
k = alpha * (1:m)/m
which(train.p.value <= k)
train.t.stat = predict(orderNorm_obj)
train.p.value = 2*(1 - pnorm(abs(train.t.stat)))
x = seq(-8,8,by=0.1)
hist(train.t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
train.p.value
#estimation of p0
alpha = 0.2
m = length(train.p.value)
k = alpha * (1:m)/m
which(train.p.value <= k)
plot(train.p.value)
lines(k, lty=2)
plot(train.p.value[1:10])
lines(k, lty=2)
k
p0 = 1
P0 = train.p.value
Phat = ecdf(train.p.value)(p.value)
FDRhat = p0 * P0 / pmax(Phat,1/m)
plot(p.value[1:10], m*Phat[1:10], "b", ylim=c(0,10))
lines(p.value[1:10],m*P0[1:10], lty=2)
legend(0,10,c("m*Phat","m*P0"),lty=c(1,2))
plot(p.value[1:10], FDRhat[1:10],"b")
abline(a=alpha0, b=0, lty=2)
p0 = 1
P0 = train.p.value
Phat = ecdf(train.p.value)(p.value)
FDRhat = p0 * P0 / pmax(Phat,1/m)
plot(p.value[1:10], m*Phat[1:10], "b", ylim=c(0,10))
lines(p.value[1:10],m*P0[1:10], lty=2)
legend(0,10,c("m*Phat","m*P0"),lty=c(1,2))
plot(p.value[1:10], FDRhat[1:10],"b")
abline(a=alpha, b=0, lty=2)
p0 = 1
P0 = train.p.value
Phat = ecdf(train.p.value)(train.p.value)
FDRhat = p0 * P0 / pmax(Phat,1/m)
plot(p.value[1:10], m*Phat[1:10], "b", ylim=c(0,10))
lines(p.value[1:10],m*P0[1:10], lty=2)
legend(0,10,c("m*Phat","m*P0"),lty=c(1,2))
plot(p.value[1:10], FDRhat[1:10],"b")
abline(a=alpha, b=0, lty=2)
Phat
mean(Phat)
mean(train.p.value)
pmax(Phat, 1/m)
mean(pmax(Phat, 1/m))
stderr(pmax(Phat, 1/m))
stdev(pmax(Phat, 1/m))
var(pmax(Phat, 1/m))
var(train.p.value)
names(df2)
actual
names(df2[g1])
setwd('..')
setwd('..')
datadir = "data/cleaned"
df1 = read.table(file=paste(datadir,"golub1",sep='/'))
df2 = read.table(file=paste(datadir,"golub2",sep='/'))
#test set
g1 = 3:22 #ALL
g2 = 23:36 #AML
t.stat = apply(X=df1, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$statistic})
df = apply(X=df1, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$parameter})
p.value = 2*(1 - pnorm(abs(t.stat)))
#p values histogram
p = seq(0,1,by=0.01)
hist(p.value, freq=F)
lines(p, dunif(p), lwd=2, col='red')
#t statistics histograms
x = seq(-8,8,by=0.1)
hist(t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
#training set
g1 = 3:29 #ALL
g2 = 30:40 #AML
t.stat = apply(X=df2, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$statistic})
p.value = 2*(1 - pnorm(abs(t.stat)))
#p values histogram
p = seq(0,1,by=0.01)
hist(p.value, freq=F)
lines(p, dunif(p), lwd=2, col='red')
#t statistics histogram
x = seq(-8,8,by=0.1)
hist(t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
#every t-statistic has a different number of df, transformation should be different for each
#if this works then next we can simulate the null proportion
orderNorm_obj = orderNorm(t.stat) #currently working w/ training data
train.t.stat = predict(orderNorm_obj)
train.p.value = 2*(1 - pnorm(abs(train.t.stat)))
x = seq(-8,8,by=0.1)
hist(train.t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
# Expected number of p-values less than a threshold
p.expected = p*m
p.obtained = rep(0, length(p))
for(i in 1:length(p)) {
p.obtained[i] = sum(p.value < p[i])
}
plot(p, p.obtained, col='black', type='l', lwd=2, ylab='# p-values')
lines(p, p.expected, type='l', col='red', lwd=2)
# Expected number of t-values greater than a threshold
t.expected = m*2*(1-pnorm(abs(x)))
t.obtained = rep(0, length(x))
for(i in 1:length(x)) {
t.obtained[i] = sum(abs(t.stat) > abs(x[i]))
}
plot(x, t.obtained, col='black', type='l', lwd=2, ylab='# t-stat')
lines(x, t.expected, type='l', col='red', lwd=2)
# Plot of false discovery rate
plot(p, p.expected/p.obtained, col='black', type='l', lwd=2, ylab='FDR')
plot(x, t.expected/t.obtained, col='black', type='l', lwd=2, ylab='FDR')
#estimation of p0
alpha = 0.2
m = length(train.p.value)
k = alpha * (1:m)/m
p0 = 1
P0 = train.p.value
Phat = ecdf(train.p.value)(train.p.value)
var(pmax(Phat, 1/m))
var(train.p.value)
FDRhat = p0 * P0 / pmax(Phat,1/m)
plot(p.value[1:10], m*Phat[1:10], "b", ylim=c(0,10))
lines(p.value[1:10],m*P0[1:10], lty=2)
legend(0,10,c("m*Phat","m*P0"),lty=c(1,2))
plot(p.value[1:10], FDRhat[1:10],"b")
abline(a=alpha, b=0, lty=2)
# Expected number of p-values less than a threshold
m = length(train.p.value)
setwd("~/Documents/GitHub/dsc180b-capstone/src/analysis")
setwd('..')
setwd('..')
datadir = "data/cleaned"
df1 = read.table(file=paste(datadir,"golub1",sep='/'))
df2 = read.table(file=paste(datadir,"golub2",sep='/'))
#test set
g1 = 3:22 #ALL
g2 = 23:36 #AML
t.stat = apply(X=df1, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$statistic})
df = apply(X=df1, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$parameter})
p.value = 2*(1 - pnorm(abs(t.stat)))
#p values histogram
p = seq(0,1,by=0.01)
hist(p.value, freq=F)
lines(p, dunif(p), lwd=2, col='red')
#t statistics histograms
x = seq(-8,8,by=0.1)
hist(t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
#training set
g1 = 3:29 #ALL
g2 = 30:40 #AML
t.stat = apply(X=df2, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$statistic})
p.value = 2*(1 - pnorm(abs(t.stat)))
#p values histogram
p = seq(0,1,by=0.01)
hist(p.value, freq=F)
lines(p, dunif(p), lwd=2, col='red')
#t statistics histogram
x = seq(-8,8,by=0.1)
hist(t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
#every t-statistic has a different number of df, transformation should be different for each
#if this works then next we can simulate the null proportion
orderNorm_obj = orderNorm(t.stat) #currently working w/ training data
train.t.stat = predict(orderNorm_obj)
train.p.value = 2*(1 - pnorm(abs(train.t.stat)))
x = seq(-8,8,by=0.1)
hist(train.t.stat, breaks=x, freq=F)
lines(x, dnorm(x), lwd=2, col='red')
# Expected number of p-values less than a threshold
m = length(train.p.value)
p.expected = p*m
p.obtained = rep(0, length(p))
for(i in 1:length(p)) {
p.obtained[i] = sum(train.p.value < p[i])
}
plot(p, p.obtained, col='black', type='l', lwd=2, ylab='# p-values')
lines(p, p.expected, type='l', col='red', lwd=2)
# Expected number of t-values greater than a threshold
t.expected = m*2*(1-pnorm(abs(x)))
t.obtained = rep(0, length(x))
for(i in 1:length(x)) {
t.obtained[i] = sum(abs(t.stat) > abs(x[i]))
}
plot(x, t.obtained, col='black', type='l', lwd=2, ylab='# t-stat')
lines(x, t.expected, type='l', col='red', lwd=2)
# Plot of false discovery rate
plot(p, p.expected/p.obtained, col='black', type='l', lwd=2, ylab='FDR')
plot(x, t.expected/t.obtained, col='black', type='l', lwd=2, ylab='FDR')
#estimation of p0
p.expected/p.obtained
p.obtained
# Expected number of p-values less than a threshold
m = length(train.p.value)
p.expected = p*m
p.obtained = rep(0, length(p))
for(i in 1:length(p)) {
p.obtained[i] = sum(train.p.value < p[i])
}
plot(p, p.obtained, col='black', type='l', lwd=2, ylab='# p-values')
lines(p, p.expected, type='l', col='red', lwd=2)
orderNorm_obj$norm_stat
orderNorm_obj$ties_status
orderNorm_obj$x.t
mse=(square(2))
mse=square(2)
mse=(train.t.stat - mean(train.t.stat))^2
mse=mean((train.t.stat - mean(train.t.stat))^2)
mse
help(t.test)
p.value = apply(X=df2, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$p.value})
p = seq(0,1,by=0.01)
hist(p.value, freq=F)
lines(p, dunif(p), lwd=2, col='red')
t.stat > 0
sum(t.stat > 0)
sum(t.stat < 0)
t.stat = apply(X=df2, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$statistic})
df = apply(X=df2, MARGIN=1, FUN=function(X){t.test(as.numeric(X[g1]),as.numeric(X[g2]))$df})
